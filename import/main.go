package main

import (
	"bufio"
	"context"
	"database/sql"
	"encoding/binary"
	"io"
	"io/ioutil"
	"log"
	"net"
	"net/http"
	"os"
	"regexp"
	"strconv"
	"time"

	"github.com/go-redis/redis/v8"
	"github.com/jasonlvhit/gocron"

	_ "github.com/go-sql-driver/mysql"
	"gopkg.in/yaml.v2"
)

type feeds struct {
	Feeds []feed `yaml:"feeds"`
}

type feed struct {
	Name string `yaml:"name"`
	URL  string `yaml:"url"`
	Path string
}

//DB to oen DB connections
var DB *sql.DB

var ctx = context.Background()

//InitDB starts up the DB pool
func InitDB(dsn string) {
	var err error

	DB, err = sql.Open("mysql", dsn)
	err = DB.Ping()
	if err != nil {
		log.Println("Pausing 30s for DB")
		time.Sleep(30 * time.Second)
		DB, err = sql.Open("mysql", dsn)
		err = DB.Ping()
		if err != nil {
			log.Println("Pausing 30s for DB")
			time.Sleep(30 * time.Second)
			DB, err = sql.Open("mysql", dsn)
		}
	}

	//DB.SetConnMaxLifetime(time.Second * 20)

	if err = DB.Ping(); err != nil {
		log.Fatalf("Dieing: %v", err)
	}

}

func ip2int(ip net.IP) uint32 {
	if len(ip) == 16 {
		return binary.BigEndian.Uint32(ip[12:16])
	}
	return binary.BigEndian.Uint32(ip)
}

func int2ip(nn uint32) net.IP {
	ip := make(net.IP, 4)
	binary.BigEndian.PutUint32(ip, nn)
	return ip
}

// getFeeds pulls the list of feeds to parse from a yaml file.
// List contains the name and url for the feed
func (f *feeds) getFeeds(filepath string) error {
	yamlFile, err := ioutil.ReadFile(filepath)
	if err != nil {
		log.Printf("yamlFile.Get err   %v ", err)
	}
	err = yaml.Unmarshal(yamlFile, f)
	if err != nil {
		log.Fatalf("Unmarshal: %v", err)
	}
	return err
}

// GetIntel function to download an Intel file
func GetIntel() (err error) {
	log.Println("GetIntel Started")
	num := 0
	var f feed

	for num >= 0 {
		start := time.Now()
		f = <-GetFileChan
		filepath := "ipsets/" + f.Path
		//for f = range feed {
		log.Printf("Starting to process %v", filepath)
		//log.Println(f.Path)
		//log.Println(f.URL)
		//log.Println(f.Name)
		if _, err := os.Stat("ipsets"); os.IsNotExist(err) {
			err = os.MkdirAll("ipsets", 0755)
			if err != nil {
				log.Println(err)
			}
		}

		out, err := os.Create(filepath)
		if err != nil {
			log.Println(err)

		}
		//defer out.Close()
		//log.Println(out)

		resp, err := http.Get(f.URL)
		//log.Println(resp)
		if err != nil {
			log.Println(err)

		}

		defer resp.Body.Close()

		_, err = io.Copy(out, resp.Body)
		if err != nil {
			log.Println(err)

		}
		//}
		ParseFilesChan <- filepath
		dur := time.Since(start)
		log.Printf("Completed getting %v and it took %v", filepath, dur)
	}
	return err
}

// ParseFile takes a filepath and grabs some values out of it.
// Only for files from github.com/firehol/ipsets
func ParseFile() (err error) {
	log.Println("ParseFile Started")
	// #
	// # tor_exits_30d
	// #
	// # ipv4 hash:ip ipset
	// #
	// # [TorProject.org] (https://www.torproject.org) list of all
	// # current TOR exit points (TorDNSEL)
	// #
	// # Maintainer      : TorProject.org
	// # Maintainer URL  : https://www.torproject.org/
	// # List source URL : https://check.torproject.org/exit-addresses
	// # Source File Date: Tue Dec 29 08:02:26 UTC 2020
	// #
	// # Category        : anonymizers
	// # Version         : 14747
	// #
	// # This File Date  : Tue Dec 29 08:08:05 UTC 2020
	// # Update Frequency: 5 mins
	// # Aggregation     : 30 days
	// # Entries         : 2633 unique IPs
	// #
	// # Full list analysis, including geolocation map, history,
	// # retention policy, overlaps with other lists, etc.
	// # available at:
	// #
	// #  http://iplists.firehol.org/?ipset=tor_exits_30d
	// #
	// # Generated by FireHOL's update-ipsets.sh
	// # Processed with FireHOL's iprange
	var (
		//fileComment = regexp.MustCompile(`^#.*`)
		fileTag = regexp.MustCompile(`#\s\shttp\://iplists\.firehol\.org\/\?ipset=(.*)$`)
		//feedInfo   = regexp.MustCompile(`^#\s\s(?P<stuff>http.*)`)
		maintainer = regexp.MustCompile(`^#\sMaintainer\s\s.*:\s(?P<stuff>.*)`)
		maintURL   = regexp.MustCompile(`^#\sMaintainer\sURL\s.*:\s(?P<stuff>.*)`)
		category   = regexp.MustCompile(`^#\sCategory\s.*:\s(?P<stuff>.*)`)
		done       = regexp.MustCompile(`^#\sProcessed\swith\sFireHOL.*`)
		isip       = regexp.MustCompile(`^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$`)
		tag        string
		//infourl    string
		maint string
		murl  string
		cat   string
		fid   int64
		num   int
	)
	err = DB.Ping()

	if err != nil {
		log.Fatalf("Error: %v\n", err)
	}
	for num >= 0 {
		counter := 0
		start := time.Now()
		filepath := <-ParseFilesChan
		log.Printf("Starting parsing %v", filepath)
		file, err := os.Open(filepath)
		if err != nil {
			log.Fatal(err)
		}
		defer file.Close()

		scanner := bufio.NewScanner(file)
		for scanner.Scan() {
			counter++

			text := scanner.Text()
			switch {
			case fileTag.MatchString(text):
				tag = fileTag.FindStringSubmatch(text)[1]
				//log.Printf("\tTag: %s\n", tag)
				//log.Printf("\tInfo URL: http://iplists.firehol.org/?ipset=%s", tag)
			//case feedInfo.MatchString(text):
			//	infourl = feedInfo.FindStringSubmatch(text)[1]
			//	log.Printf("\tInfo URL: %s\n", infourl)
			case maintainer.MatchString(text):
				maint = maintainer.FindStringSubmatch(text)[1]
				//log.Printf("\tMaintainer: %s\n", maint)
			case maintURL.MatchString(text):
				murl = maintURL.FindStringSubmatch(text)[1]
				//log.Printf("\tMaintainer URL: %s\n", murl)
			case category.MatchString(text):
				cat = category.FindStringSubmatch(text)[1]
				//log.Printf("\tCategory: %s\n", cat)
			case done.MatchString(text):
				infourl := "http://iplists.firehol.org/?ipset=" + tag
				stmt, err := DB.Prepare("INSERT INTO feeds (tag, category, feed_name, feed_url, info_url) VALUES (?, ?, ?, ?, ?) ON DUPLICATE KEY UPDATE tag=VALUES(tag), category=VALUES(category), feed_name=VALUES(feed_name), feed_url=VALUES(feed_url), info_url=VALUES(info_url), updated=CURRENT_TIMESTAMP()")
				if err != nil {
					log.Fatal(err)
				}

				res, err := stmt.Exec(tag, cat, maint, murl, infourl)
				if err != nil {
					log.Println(err)
					res, err = stmt.Exec(tag, cat, maint, murl, infourl)
				}
				fid, err = res.LastInsertId()
				if err != nil {
					log.Fatal(err)
				}
				//log.Printf("\tFeed id: %v\n", fid)
				stmt.Close()
			case isip.MatchString(text):
				var ipid int64
				ipaddr := ip2int(net.ParseIP(text))

				// insert / update each ip, get last id as ip_id
				stmt, err := DB.Prepare("INSERT INTO ip (ipaddr) VALUES (?) ON DUPLICATE KEY UPDATE updated=CURRENT_TIMESTAMP()")
				if err != nil {
					log.Fatal(err)
				}

				res, err := stmt.Exec(ipaddr)
				if err != nil {
					log.Println(err)
					res, err = stmt.Exec(ipaddr)
				}
				ipid, err = res.LastInsertId()
				if err != nil {
					log.Fatal(err)
				}
				//log.Printf("\tIP : %v %v %v %v", scanner.Text(), ipaddr, ipid, id)
				//insert each entry as intel with feed_id and ip_id
				//log.Printf("\tIP: %v\n", scanner.Text())
				stmt.Close()
				stmt, err = DB.Prepare("INSERT INTO intel (ip_id, feed_id) VALUES (?, ?) ON DUPLICATE KEY UPDATE updated=CURRENT_TIMESTAMP()")
				if err != nil {
					log.Fatal(err)
				}

				res, err = stmt.Exec(ipid, fid)
				if err != nil {
					log.Println(err)
					res, err = stmt.Exec(ipid, fid)

				}
				stmt.Close()
			default:
			}

			if counter%5000 == 0 {
				log.Printf("\tProcessed: %v of %v", counter, filepath)
			}

		}
		dur := time.Since(start)
		log.Printf("Completed %v with feed_id %v in %v", filepath, fid, dur)
		GetData()
		DoneChan <- true
	}
	return nil
}

//GetFeedList iterates over the supplied yaml pulling out the information on the feeds
func GetFeedList(yamlPath string) error {

	// sqlChan := make(chan string, 2)
	// importDone := make(chan bool, 1)
	// sqlResult := make(chan string, 5)
	// redisSetChan := make(chan string, 2)

	var f feeds
	var fileName = regexp.MustCompile(`\A.*\/(?P<filename>.*)`)
	err := f.getFeeds(yamlPath)
	if err != nil {
		log.Fatalf("Error: %v", err)
	}
	for _, feed := range f.Feeds {

		//log.Printf("Name: %v\n", feed.Name)
		//log.Printf("\tURL: %v\n", feed.URL)
		filename := fileName.FindStringSubmatch(feed.URL)
		//log.Printf("\tFilename: %v\n", filename[1])
		feed.Path = filename[1]
		GetFileChan <- feed
		//ParseFile("ipsets/"+filename[1], id+1)
	}
	return err
}

func logtime() {
	log.Printf(".")
}

// PutRedis takes an ip and the json string and puts it in redis
func PutRedis(ip string, json string) (err error) {
	client := redis.NewClient(&redis.Options{
		Addr:               "redis-master:6379",
		Password:           "",
		DB:                 0,
		IdleTimeout:        -1,
		MaxRetries:         3,
		PoolSize:           100,
		IdleCheckFrequency: -1,
	})
	defer client.Close()
	//log.Printf("Key: %v Value: %v", ip, json)
	err = client.Set(ctx, ip, json, 0).Err()
	if err != nil {
		log.Println(err)
		return err
	}

	return nil

}

// Stage2 is a goroutine to kick off stage 2
func Stage2() {
	for {
		done := <-DoneChan
		if done == true {
			GetData()
		}
	}
}

// GetData grabs the data out of the db and parses it to JSON
func GetData() (err error) {
	var Count int
	var ipaddr string
	var intel string
	var ips int
	var feeds int
	var ints int
	// if err != nil {
	// 	log.Fatalf("Error: %v\n", err)
	// }
	client := redis.NewClient(&redis.Options{
		Addr:       "redis-master:6379",
		Password:   "",
		DB:         0,
		MaxRetries: 3,
		PoolSize:   100,
	})
	defer client.Close()
	m := map[string]*redis.StatusCmd{}
	pipe := client.Pipeline()
	defer pipe.Close()
	log.Println("Starting Mysql -> Redis Update")
	query := `SELECT 
    INET_NTOA(ip.ipaddr) ip,
    CONCAT('{"ip":"',
            INET_NTOA(ip.ipaddr),
            '","created":"',
            ip.created,
            '","updated":"',
            ip.updated,
            '","intel":[',
            GROUP_CONCAT(CONCAT('{"tag":"',
                        feeds.tag,
                        '", "cat":"',
                        feeds.category,
                        '", "name":"',
                        feeds.tag,
                        '", "url":"',
                        feeds.feed_url,
                        '",  "created":"',
                        feeds.created,
                        '",  "updated":"',
                        feeds.updated,
                        '"}')),
            ']}') intel
	FROM
		itip.intel
			LEFT OUTER JOIN
		itip.ip ON intel.ip_id = ip.id
			LEFT OUTER JOIN
		itip.feeds ON intel.feed_id = feeds.id
	GROUP BY intel.ip_id;`
	stmt, err := DB.Query(query)
	if err != nil {
		log.Fatal(err)
	}
	defer stmt.Close()
	Count = 0
	for stmt.Next() {
		Count++
		err = stmt.Scan(&ipaddr, &intel)
		if err != nil {
			log.Println(err)
		}
		// err = PutRedis(ipaddr, intel)
		m[ipaddr] = pipe.Set(ctx, ipaddr, intel, -1)
		//
		if Count%5000 == 0 {
			_, err := pipe.Exec(ctx)
			if err != nil {
				log.Println(err)
			}
			log.Printf("Updated %v in Redis", Count)
		}
	}
	log.Println("comopleted Redis Update\nStarting to update status")
	statusq := `SELECT COUNT(id) as ips,(SELECT COUNT(id) FROM itip.feeds) as feeds,(SELECT COUNT(id) FROM itip.intel) as intel FROM itip.ip;`
	stmt, err = DB.Query(statusq)
	if err != nil {
		log.Fatal(err)
	}
	defer stmt.Close()
	for stmt.Next() {
		err = stmt.Scan(&ips, &feeds, &ints)
		if err != nil {
			log.Println(err)
		}
		status := "{\"ips\":\"" + strconv.Itoa(ips) + "\",\"feeds\":\"" + strconv.Itoa(feeds) + "\",\"intel\":\"" + strconv.Itoa(ints) + "\",\"updated\":\"" + time.Now().Format(time.RFC3339) + "\"}"
		err = PutRedis("Status", status)
		if err != nil {
			log.Println(err)
		}
		log.Printf("Updated Status : %v", status)
	}

	log.Println(Count)
	if err != nil {
		log.Fatal(err)
	}

	return err
}

// Channels

// GetFileChan to submit filename
var GetFileChan = make(chan feed, 5)

// ParseFilesChan to parse file
var ParseFilesChan = make(chan string, 5)

// DoneChan to signal intel import is done.
var DoneChan = make(chan bool, 1)

func main() {
	// go func() {
	// 	for {
	// 		var m runtime.MemStats
	// 		runtime.ReadMemStats(&m)
	// 		log.Printf("Alloc = %v\tTotalAlloc = %v\tSys = %v\tNumGC = %v\tGoRoutines = %v\tStop-Pause-ns = %v\tLast-Pause = %v", m.Alloc/1024, m.TotalAlloc/1024, m.Sys/1024, m.NumGC, runtime.NumGoroutine(), m.PauseTotalNs, m.PauseNs[0])
	// 		time.Sleep(5 * time.Second)
	// 	}
	// }()
	var vers string
	defer close(GetFileChan)
	defer close(ParseFilesChan)
	defer close(DoneChan)
	log.SetFlags(log.LstdFlags | log.Lmicroseconds | log.Lshortfile)
	log.Printf("Intel Import Started")
	InitDB("itip:#itip2017@(mysql:3306)/itip?parseTime=true&timeout=90s")
	defer DB.Close()
	ver, err := DB.Query("select @@version as vers")
	if err != nil {
		log.Printf("Error: %v\n", err)
	}
	defer ver.Close()
	for ver.Next() {
		err := ver.Scan(&vers)
		if err != nil {
			log.Fatal(err)
		}
		log.Printf("MySQL Version: %v\n", vers)
	}
	err = ver.Err()
	if err != nil {
		log.Fatal(err)
	}
	// Update Redis
	go GetIntel()
	go ParseFile()
	go Stage2()

	//gocron.Every(10).Seconds().Do(logtime)
	start := time.Now()
	GetFeedList("feeds.yaml")
	dur := time.Since(start)
	log.Printf("Took %v to do first GetFeedList", dur)
	gocron.Every(60).Minutes().Do(GetFeedList, "feeds.yaml")
	_, nxtime := gocron.NextRun()
	log.Printf("Next Run is %v", nxtime.Format(time.RFC3339))
	gocron.RunPending()
	<-gocron.Start()
}

// Todo:
// - Better control the goroutines associated with reading in and parsing the intel feeds
// - Make feeds.yml be monitored for changes, and reprocess feeds on change
// - Check the updated time of the feed and compare against updated time in DB, only update feed if it's been updated.
